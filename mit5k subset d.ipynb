{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a733f38-4673-454a-96d8-7989944662e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FA007\\.conda\\envs\\new2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\FA007\\.conda\\envs\\new2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancing: D:\\mit-5k-subset\\d\\a0034-LSYD4O2202.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from deap import base, creator, tools\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Setup Pretrained Feature Extractor with PyTorch (GPU)\n",
    "# ------------------------------\n",
    "class VGG16FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16FeatureExtractor, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = vgg16.features\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor = VGG16FeatureExtractor().to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Transformation for feature extractor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_features_torch(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Extract deep features from an RGB image (numpy array [0,1])\"\"\"\n",
    "    input_tensor = transform((img * 255).astype(np.uint8))\n",
    "    input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = feature_extractor(input_tensor)\n",
    "    return feats.cpu().numpy().flatten()\n",
    "\n",
    "# ------------------------------\n",
    "# 2. GPU-based Image Enhancement Operators\n",
    "# ------------------------------\n",
    "def enhance_image_gpu(image: np.ndarray, individual: list) -> np.ndarray:\n",
    "    brightness, contrast, gamma = individual\n",
    "    img_tensor = torch.tensor(image, dtype=torch.float32, device=device)\n",
    "    img_bright = torch.clamp(img_tensor + (brightness / 255.0), 0.0, 1.0)\n",
    "    img_contrast = torch.clamp(contrast * img_bright, 0.0, 1.0)\n",
    "    img_gamma = torch.clamp(torch.pow(img_contrast, 1.0 / gamma), 0.0, 1.0)\n",
    "    return img_gamma.cpu().numpy()\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Corrected Fitness Evaluation\n",
    "# ------------------------------\n",
    "def evaluate_individual(individual, original_img, orig_features):\n",
    "    enhanced = enhance_image_gpu(original_img, individual)\n",
    "    img_tensor = torch.tensor(enhanced, dtype=torch.float32, device=device)\n",
    "    gray = 0.114 * img_tensor[:,:,0] + 0.587 * img_tensor[:,:,1] + 0.299 * img_tensor[:,:,2]\n",
    "    hist = torch.histc(gray, bins=256, min=0.0, max=1.0)\n",
    "    prob = hist / torch.sum(hist)\n",
    "    entropy = -torch.sum(prob * torch.log2(prob + 1e-6)).item()\n",
    "\n",
    "    # Deep features\n",
    "    enhanced_rgb = cv2.cvtColor((enhanced * 255).astype(np.uint8), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    feat_enh = extract_features_torch(enhanced_rgb)\n",
    "    feat_dist = np.linalg.norm(orig_features - feat_enh)\n",
    "\n",
    "    # Brightness penalty\n",
    "    mean_b = torch.mean(img_tensor).item()\n",
    "    penalty = 0.0\n",
    "    low, up, factor = 0.35, 0.7, 30.0\n",
    "    if mean_b < low:\n",
    "        penalty += (low - mean_b) * factor\n",
    "    elif mean_b > up:\n",
    "        penalty += (mean_b - up) * factor\n",
    "\n",
    "    return entropy, feat_dist + penalty\n",
    "\n",
    "# ------------------------------\n",
    "# 4. GA Configuration\n",
    "# ------------------------------\n",
    "PARAM_BOUNDS = [(-10, 60), (1.0, 2.0), (1.0, 2.0)]\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "for i, (low, up) in enumerate(PARAM_BOUNDS):\n",
    "    toolbox.register(f\"attr_{i}\", random.uniform, low, up)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.attr_0, toolbox.attr_1, toolbox.attr_2), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate_individual)\n",
    "\n",
    "def bounded_cx(ind1, ind2):\n",
    "    tools.cxBlend(ind1, ind2, alpha=0.5)\n",
    "    for i, (low, up) in enumerate(PARAM_BOUNDS):\n",
    "        ind1[i] = np.clip(ind1[i], low, up)\n",
    "        ind2[i] = np.clip(ind2[i], low, up)\n",
    "    return ind1, ind2\n",
    "\n",
    "def bounded_mut(ind, indpb):\n",
    "    for i, (low, up) in enumerate(PARAM_BOUNDS):\n",
    "        if random.random() < indpb:\n",
    "            ind[i] += random.gauss(0, (up - low) * 0.1)\n",
    "            ind[i] = np.clip(ind[i], low, up)\n",
    "    return ind,\n",
    "\n",
    "toolbox.register(\"mate\", bounded_cx)\n",
    "toolbox.register(\"mutate\", bounded_mut, indpb=0.4)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Local Search\n",
    "# ------------------------------\n",
    "def local_search(ind, original_img, orig_features, max_iter=8):\n",
    "    best = ind[:]\n",
    "    best_fit = evaluate_individual(best, original_img, orig_features)\n",
    "    for _ in range(max_iter):\n",
    "        neigh = [np.clip(p + random.uniform(-0.5, 0.5), lo, hi)\n",
    "                 for p, (lo, hi) in zip(best, PARAM_BOUNDS)]\n",
    "        fit_n = evaluate_individual(neigh, original_img, orig_features)\n",
    "        if (fit_n[0] > best_fit[0]) or (fit_n[0] == best_fit[0] and fit_n[1] < best_fit[1]):\n",
    "            best, best_fit = neigh, fit_n\n",
    "    return best\n",
    "\n",
    "# ------------------------------\n",
    "# 6. GA Routine Without Display\n",
    "# ------------------------------\n",
    "def run_ga(original_img, ngen=35, pop_size=50):\n",
    "    orig_f = original_img.astype(np.float32) / 255.0\n",
    "    orig_rgb = cv2.cvtColor((orig_f * 255).astype(np.uint8), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    feats = extract_features_torch(orig_rgb)\n",
    "\n",
    "    pop = toolbox.population(n=pop_size)\n",
    "    for ind in pop:\n",
    "        ind.fitness.values = toolbox.evaluate(ind, orig_f, feats)\n",
    "    pop = toolbox.select(pop, len(pop))\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        offspring = tools.selTournamentDCD(pop, len(pop))\n",
    "        offspring = [toolbox.clone(i) for i in offspring]\n",
    "        # Crossover & Mutation\n",
    "        for i1, i2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < 0.85:\n",
    "                toolbox.mate(i1, i2)\n",
    "                del i1.fitness.values, i2.fitness.values\n",
    "        mut_rate = 0.3 if gen < ngen//2 else 0.2\n",
    "        for m in offspring:\n",
    "            if random.random() < mut_rate:\n",
    "                toolbox.mutate(m)\n",
    "                del m.fitness.values\n",
    "\n",
    "        invalid = [i for i in offspring if not i.fitness.valid]\n",
    "        for i in invalid:\n",
    "            i.fitness.values = toolbox.evaluate(i, orig_f, feats)\n",
    "        pop = toolbox.select(pop + offspring, pop_size)\n",
    "\n",
    "        # Local search on top 20%\n",
    "        top = tools.selBest(pop, max(1, int(0.2 * pop_size)))\n",
    "        for ind in top:\n",
    "            new_p = local_search(ind, orig_f, feats)\n",
    "            new_fit = evaluate_individual(new_p, orig_f, feats)\n",
    "            if (new_fit[0] > ind.fitness.values[0]) or \\\n",
    "               (new_fit[0] == ind.fitness.values[0] and new_fit[1] < ind.fitness.values[1]):\n",
    "                ind[:] = new_p\n",
    "                ind.fitness.values = new_fit\n",
    "\n",
    "    best = tools.selBest(pop, 1)[0]\n",
    "    enhanced = enhance_image_gpu(orig_f, best)\n",
    "    return enhanced\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Batch Processing\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    input_root = r\"D:\\mit-5k-subset\\d\"\n",
    "    output_root = r\"D:\\mit-5k-subset\\d_enhanced\"\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "    for subdir, _, files in os.walk(input_root):\n",
    "        rel_path = os.path.relpath(subdir, input_root)\n",
    "        output_subdir = os.path.join(output_root, rel_path)\n",
    "        os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "        for fname in files:\n",
    "            if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\")):\n",
    "                continue\n",
    "            in_path = os.path.join(subdir, fname)\n",
    "            img = cv2.imread(in_path)\n",
    "            if img is None:\n",
    "                print(f\"Warning: unable to read {in_path}\")\n",
    "                continue\n",
    "            print(f\"Enhancing: {in_path}\")\n",
    "            enhanced = run_ga(img, ngen=5, pop_size=60)\n",
    "            out_img = (enhanced * 255).astype(np.uint8)\n",
    "            out_path = os.path.join(output_subdir, fname)\n",
    "            cv2.imwrite(out_path, out_img)\n",
    "            print(f\"Saved enhanced image: {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0049af-bc78-4329-aad8-cb40ed116b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FA007\\.conda\\envs\\new2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\FA007\\.conda\\envs\\new2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0034-LSYD4O2202.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0035-dgw_048.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0291-IMG_0115.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0436-IMG_2583.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0452-IMG_1646.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0463-jmac_DSC2316.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0631-NKIM_MG_6442.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0648-IMG_5085.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0712-_DSC8911.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0752-20061213_134314__MG_3708.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0767-jn_20070824_0165.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0770-050703_161554__I2E9266.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0775-kme_423.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0811-20051224_165428__MG_0953.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0843-IMG_0009.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0877-_DGW6231.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0944-20061213_132310__MG_3646.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a0982-jmac_MG_1105.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1085-_DSC6188.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1099-IMG_3511.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1125-07-11-25-at-10h33m49s-_MG_6884.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1160-_MG_1159.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1194-IMG_0479.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1219-IMG_3770.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1268-jmac_MG_5989.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1319-IMG_2601.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1352-07-11-04-at-17h58m48s-_MG_4012.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1368-kme_086.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1439-IMG_0055.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1454-DSC_0028-1.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1606-kme_447.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1624-_DSC0158.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1659-IMG_4181.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1693-IMG_5069.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1701-jmac_DSC3938.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1717-kme_510.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1741-kme_305.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1775-dvf_006.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1789-jmac_DSC6275.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a1806-kme_342.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2031-WP_CRW_5715.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2106-20041009_164255__E6B5580.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2215-DSC_0018.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2224-MB_070908_032.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2236-jn_2007_12_10__BU_CRUISE_004.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2314-20080426_111248__MG_9227.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2398-051007_160936__I2E8858.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2416-_DGW6256.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2473-kme_303.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2499-_DSC1385.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2504-kme_370.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2560-MB_070908_079.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2713-kme_337.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2756-kme_506.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2761-kme_607.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2773-jmac_MG_4982.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2835-IMG_4968.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2879-IMG_0481.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a2881-20070514_162430__MG_7345.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3134-IMG_4250.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3193-KE_-2155.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3304-IMG_4322.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3311-IMG_4500.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3330-DSC_0048.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3360-IMG_0017.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3384-kme_524.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3411-_DGW6385.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3590-IMG_0622.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3697-07-11-24-at-16h05m35s-_MG_6729.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3732-_DGW6272.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3739-KE_-0068.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3741-KE_-8337.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3753-dgw_073.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3768-IMG_1641.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3790-IMG_5048.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3831-jmac_MG_5861.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a3959-dvf_057.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4062-IMG_1820.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4064-07-12-02-at-16h23m18s-_MG_9020.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4074-kme_428.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4090-IMG_4996.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4128-kme_388.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4225-Duggan_081109_3031.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4226-kme_0501.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4336-09-05-17-at-15h52m34s-_MG_9211.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4428-DSC_0031.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4454-DSC_0101.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4465-DSC_0051.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4501-DSC_0354.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4528-Duggan_090209_4971.jpg\n",
      "Skipping (already processed): D:\\mit-5k-subset\\d_enhanced\\.\\a4557-DSC_0477.jpg\n",
      "Enhancing: D:\\mit-5k-subset\\d\\a4573-kme_0734.jpg\n",
      "Saved enhanced image: D:\\mit-5k-subset\\d_enhanced\\.\\a4573-kme_0734.jpg\n",
      "Enhancing: D:\\mit-5k-subset\\d\\a4635-DSC_0017.jpg\n",
      "Saved enhanced image: D:\\mit-5k-subset\\d_enhanced\\.\\a4635-DSC_0017.jpg\n",
      "Enhancing: D:\\mit-5k-subset\\d\\a4662-Duggan_080115_4605.jpg\n",
      "Saved enhanced image: D:\\mit-5k-subset\\d_enhanced\\.\\a4662-Duggan_080115_4605.jpg\n",
      "Enhancing: D:\\mit-5k-subset\\d\\a4673-CRW_0009.jpg\n",
      "Saved enhanced image: D:\\mit-5k-subset\\d_enhanced\\.\\a4673-CRW_0009.jpg\n",
      "Enhancing: D:\\mit-5k-subset\\d\\a4706-DSC_0066.jpg\n",
      "Saved enhanced image: D:\\mit-5k-subset\\d_enhanced\\.\\a4706-DSC_0066.jpg\n",
      "Enhancing: D:\\mit-5k-subset\\d\\a4826-Duggan_080821_1199.jpg\n",
      "Saved enhanced image: D:\\mit-5k-subset\\d_enhanced\\.\\a4826-Duggan_080821_1199.jpg\n",
      "Enhancing: D:\\mit-5k-subset\\d\\a4828-Duggan_050710_1299.jpg\n",
      "Saved enhanced image: D:\\mit-5k-subset\\d_enhanced\\.\\a4828-Duggan_050710_1299.jpg\n",
      "Enhancing: D:\\mit-5k-subset\\d\\a4891-DSC_0069.jpg\n",
      "Saved enhanced image: D:\\mit-5k-subset\\d_enhanced\\.\\a4891-DSC_0069.jpg\n",
      "Enhancing: D:\\mit-5k-subset\\d\\a4943-20090327_at_11h17m20__MG_0987.jpg\n",
      "Saved enhanced image: D:\\mit-5k-subset\\d_enhanced\\.\\a4943-20090327_at_11h17m20__MG_0987.jpg\n"
     ]
    }
   ],
   "source": [
    "#run this cell only if you stop the first cell without completition\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from deap import base, creator, tools\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Setup Pretrained Feature Extractor with PyTorch (GPU)\n",
    "# ------------------------------\n",
    "class VGG16FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16FeatureExtractor, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = vgg16.features\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor = VGG16FeatureExtractor().to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Transformation for feature extractor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_features_torch(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Extract deep features from an RGB image (numpy array [0,1])\"\"\"\n",
    "    input_tensor = transform((img * 255).astype(np.uint8))\n",
    "    input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = feature_extractor(input_tensor)\n",
    "    return feats.cpu().numpy().flatten()\n",
    "\n",
    "# ------------------------------\n",
    "# 2. GPU-based Image Enhancement Operators\n",
    "# ------------------------------\n",
    "def enhance_image_gpu(image: np.ndarray, individual: list) -> np.ndarray:\n",
    "    brightness, contrast, gamma = individual\n",
    "    img_tensor = torch.tensor(image, dtype=torch.float32, device=device)\n",
    "    img_bright = torch.clamp(img_tensor + (brightness / 255.0), 0.0, 1.0)\n",
    "    img_contrast = torch.clamp(contrast * img_bright, 0.0, 1.0)\n",
    "    img_gamma = torch.clamp(torch.pow(img_contrast, 1.0 / gamma), 0.0, 1.0)\n",
    "    return img_gamma.cpu().numpy()\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Corrected Fitness Evaluation\n",
    "# ------------------------------\n",
    "def evaluate_individual(individual, original_img, orig_features):\n",
    "    enhanced = enhance_image_gpu(original_img, individual)\n",
    "    img_tensor = torch.tensor(enhanced, dtype=torch.float32, device=device)\n",
    "    gray = 0.114 * img_tensor[:,:,0] + 0.587 * img_tensor[:,:,1] + 0.299 * img_tensor[:,:,2]\n",
    "    hist = torch.histc(gray, bins=256, min=0.0, max=1.0)\n",
    "    prob = hist / torch.sum(hist)\n",
    "    entropy = -torch.sum(prob * torch.log2(prob + 1e-6)).item()\n",
    "\n",
    "    # Deep features\n",
    "    enhanced_rgb = cv2.cvtColor((enhanced * 255).astype(np.uint8), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    feat_enh = extract_features_torch(enhanced_rgb)\n",
    "    feat_dist = np.linalg.norm(orig_features - feat_enh)\n",
    "\n",
    "    # Brightness penalty\n",
    "    mean_b = torch.mean(img_tensor).item()\n",
    "    penalty = 0.0\n",
    "    low, up, factor = 0.35, 0.7, 30.0\n",
    "    if mean_b < low:\n",
    "        penalty += (low - mean_b) * factor\n",
    "    elif mean_b > up:\n",
    "        penalty += (mean_b - up) * factor\n",
    "\n",
    "    return entropy, feat_dist + penalty\n",
    "\n",
    "# ------------------------------\n",
    "# 4. GA Configuration\n",
    "# ------------------------------\n",
    "PARAM_BOUNDS = [(-10, 60), (1.0, 2.0), (1.0, 2.0)]\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "for i, (low, up) in enumerate(PARAM_BOUNDS):\n",
    "    toolbox.register(f\"attr_{i}\", random.uniform, low, up)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.attr_0, toolbox.attr_1, toolbox.attr_2), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate_individual)\n",
    "\n",
    "def bounded_cx(ind1, ind2):\n",
    "    tools.cxBlend(ind1, ind2, alpha=0.5)\n",
    "    for i, (low, up) in enumerate(PARAM_BOUNDS):\n",
    "        ind1[i] = np.clip(ind1[i], low, up)\n",
    "        ind2[i] = np.clip(ind2[i], low, up)\n",
    "    return ind1, ind2\n",
    "\n",
    "def bounded_mut(ind, indpb):\n",
    "    for i, (low, up) in enumerate(PARAM_BOUNDS):\n",
    "        if random.random() < indpb:\n",
    "            ind[i] += random.gauss(0, (up - low) * 0.1)\n",
    "            ind[i] = np.clip(ind[i], low, up)\n",
    "    return ind,\n",
    "\n",
    "toolbox.register(\"mate\", bounded_cx)\n",
    "toolbox.register(\"mutate\", bounded_mut, indpb=0.4)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Local Search\n",
    "# ------------------------------\n",
    "def local_search(ind, original_img, orig_features, max_iter=8):\n",
    "    best = ind[:]\n",
    "    best_fit = evaluate_individual(best, original_img, orig_features)\n",
    "    for _ in range(max_iter):\n",
    "        neigh = [np.clip(p + random.uniform(-0.5, 0.5), lo, hi)\n",
    "                 for p, (lo, hi) in zip(best, PARAM_BOUNDS)]\n",
    "        fit_n = evaluate_individual(neigh, original_img, orig_features)\n",
    "        if (fit_n[0] > best_fit[0]) or (fit_n[0] == best_fit[0] and fit_n[1] < best_fit[1]):\n",
    "            best, best_fit = neigh, fit_n\n",
    "    return best\n",
    "\n",
    "# ------------------------------\n",
    "# 6. GA Routine Without Display\n",
    "# ------------------------------\n",
    "def run_ga(original_img, ngen=35, pop_size=50):\n",
    "    orig_f = original_img.astype(np.float32) / 255.0\n",
    "    orig_rgb = cv2.cvtColor((orig_f * 255).astype(np.uint8), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    feats = extract_features_torch(orig_rgb)\n",
    "\n",
    "    pop = toolbox.population(n=pop_size)\n",
    "    for ind in pop:\n",
    "        ind.fitness.values = toolbox.evaluate(ind, orig_f, feats)\n",
    "    pop = toolbox.select(pop, len(pop))\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        offspring = tools.selTournamentDCD(pop, len(pop))\n",
    "        offspring = [toolbox.clone(i) for i in offspring]\n",
    "        # Crossover & Mutation\n",
    "        for i1, i2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < 0.85:\n",
    "                toolbox.mate(i1, i2)\n",
    "                del i1.fitness.values, i2.fitness.values\n",
    "        mut_rate = 0.3 if gen < ngen//2 else 0.2\n",
    "        for m in offspring:\n",
    "            if random.random() < mut_rate:\n",
    "                toolbox.mutate(m)\n",
    "                del m.fitness.values\n",
    "\n",
    "        invalid = [i for i in offspring if not i.fitness.valid]\n",
    "        for i in invalid:\n",
    "            i.fitness.values = toolbox.evaluate(i, orig_f, feats)\n",
    "        pop = toolbox.select(pop + offspring, pop_size)\n",
    "\n",
    "        # Local search on top 20%\n",
    "        top = tools.selBest(pop, max(1, int(0.2 * pop_size)))\n",
    "        for ind in top:\n",
    "            new_p = local_search(ind, orig_f, feats)\n",
    "            new_fit = evaluate_individual(new_p, orig_f, feats)\n",
    "            if (new_fit[0] > ind.fitness.values[0]) or \\\n",
    "               (new_fit[0] == ind.fitness.values[0] and new_fit[1] < ind.fitness.values[1]):\n",
    "                ind[:] = new_p\n",
    "                ind.fitness.values = new_fit\n",
    "\n",
    "    best = tools.selBest(pop, 1)[0]\n",
    "    enhanced = enhance_image_gpu(orig_f, best)\n",
    "    return enhanced\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Batch Processing\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    input_root = r\"D:\\mit-5k-subset\\d\"\n",
    "    output_root = r\"D:\\mit-5k-subset\\d_enhanced\"\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "    for subdir, _, files in os.walk(input_root):\n",
    "        rel_path = os.path.relpath(subdir, input_root)\n",
    "        output_subdir = os.path.join(output_root, rel_path)\n",
    "        os.makedirs(output_subdir, exist_ok=True)\n",
    "        for fname in files:\n",
    "            if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\")):\n",
    "                continue\n",
    "            in_path = os.path.join(subdir, fname)\n",
    "            out_path = os.path.join(output_subdir, fname)\n",
    "\n",
    "            if os.path.exists(out_path):\n",
    "                print(f\"Skipping (already processed): {out_path}\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.imread(in_path)\n",
    "            if img is None:\n",
    "                 print(f\"Warning: unable to read {in_path}\")\n",
    "                 continue\n",
    "\n",
    "            print(f\"Enhancing: {in_path}\")\n",
    "            enhanced = run_ga(img, ngen=5, pop_size=60)\n",
    "            out_img = (enhanced * 255).astype(np.uint8)\n",
    "            cv2.imwrite(out_path, out_img)\n",
    "            print(f\"Saved enhanced image: {out_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269b6d5-a1a3-458a-b454-b6e5318f4c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
