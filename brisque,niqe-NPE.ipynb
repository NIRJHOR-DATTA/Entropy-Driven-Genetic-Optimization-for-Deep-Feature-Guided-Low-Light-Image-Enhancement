{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8158988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "from scipy.special import gamma\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65561e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic(x):\n",
    "    \"\"\"cubic function used for calculate_weights_indices.\"\"\"\n",
    "    absx = torch.abs(x)\n",
    "    absx2 = absx**2\n",
    "    absx3 = absx**3\n",
    "    return (1.5 * absx3 - 2.5 * absx2 + 1) * (\n",
    "        (absx <= 1).type_as(absx)) + (-0.5 * absx3 + 2.5 * absx2 - 4 * absx + 2) * (((absx > 1) *\n",
    "                                                                                     (absx <= 2)).type_as(absx))\n",
    "\n",
    "\n",
    "\n",
    "def calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n",
    "    \"\"\"Calculate weights and indices, used for imresize function.\n",
    "    Args:\n",
    "        in_length (int): Input length.\n",
    "        out_length (int): Output length.\n",
    "        scale (float): Scale factor.\n",
    "        kernel_width (int): Kernel width.\n",
    "        antialisaing (bool): Whether to apply anti-aliasing when downsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    if (scale < 1) and antialiasing:\n",
    "        # Use a modified kernel (larger kernel width) to simultaneously\n",
    "        # interpolate and antialias\n",
    "        kernel_width = kernel_width / scale\n",
    "\n",
    "    # Output-space coordinates\n",
    "    x = torch.linspace(1, out_length, out_length)\n",
    "\n",
    "    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n",
    "    # in output space maps to 0.5 in input space, and 0.5 + scale in output\n",
    "    # space maps to 1.5 in input space.\n",
    "    u = x / scale + 0.5 * (1 - 1 / scale)\n",
    "\n",
    "    # What is the left-most pixel that can be involved in the computation?\n",
    "    left = torch.floor(u - kernel_width / 2)\n",
    "\n",
    "    # What is the maximum number of pixels that can be involved in the\n",
    "    # computation?  Note: it's OK to use an extra pixel here; if the\n",
    "    # corresponding weights are all zero, it will be eliminated at the end\n",
    "    # of this function.\n",
    "    p = math.ceil(kernel_width) + 2\n",
    "\n",
    "    # The indices of the input pixels involved in computing the k-th output\n",
    "    # pixel are in row k of the indices matrix.\n",
    "    indices = left.view(out_length, 1).expand(out_length, p) + torch.linspace(0, p - 1, p).view(1, p).expand(\n",
    "        out_length, p)\n",
    "\n",
    "    # The weights used to compute the k-th output pixel are in row k of the\n",
    "    # weights matrix.\n",
    "    distance_to_center = u.view(out_length, 1).expand(out_length, p) - indices\n",
    "\n",
    "    # apply cubic kernel\n",
    "    if (scale < 1) and antialiasing:\n",
    "        weights = scale * cubic(distance_to_center * scale)\n",
    "    else:\n",
    "        weights = cubic(distance_to_center)\n",
    "\n",
    "    # Normalize the weights matrix so that each row sums to 1.\n",
    "    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n",
    "    weights = weights / weights_sum.expand(out_length, p)\n",
    "\n",
    "    # If a column in weights is all zero, get rid of it. only consider the\n",
    "    # first and last column.\n",
    "    weights_zero_tmp = torch.sum((weights == 0), 0)\n",
    "    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n",
    "        indices = indices.narrow(1, 1, p - 2)\n",
    "        weights = weights.narrow(1, 1, p - 2)\n",
    "    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n",
    "        indices = indices.narrow(1, 0, p - 2)\n",
    "        weights = weights.narrow(1, 0, p - 2)\n",
    "    weights = weights.contiguous()\n",
    "    indices = indices.contiguous()\n",
    "    sym_len_s = -indices.min() + 1\n",
    "    sym_len_e = indices.max() - in_length\n",
    "    indices = indices + sym_len_s - 1\n",
    "    return weights, indices, int(sym_len_s), int(sym_len_e)\n",
    "\n",
    "def imresize(img, scale, antialiasing=True):\n",
    "    \"\"\"imresize function same as MATLAB.\n",
    "    It now only supports bicubic.\n",
    "    The same scale applies for both height and width.\n",
    "    Args:\n",
    "        img (Tensor | Numpy array):\n",
    "            Tensor: Input image with shape (c, h, w), [0, 1] range.\n",
    "            Numpy: Input image with shape (h, w, c), [0, 1] range.\n",
    "        scale (float): Scale factor. The same scale applies for both height\n",
    "            and width.\n",
    "        antialisaing (bool): Whether to apply anti-aliasing when downsampling.\n",
    "            Default: True.\n",
    "    Returns:\n",
    "        Tensor: Output image with shape (c, h, w), [0, 1] range, w/o round.\n",
    "    \"\"\"\n",
    "    squeeze_flag = False\n",
    "    if type(img).__module__ == np.__name__:  # numpy type\n",
    "        numpy_type = True\n",
    "        if img.ndim == 2:\n",
    "            img = img[:, :, None]\n",
    "            squeeze_flag = True\n",
    "        img = torch.from_numpy(img.transpose(2, 0, 1)).float()\n",
    "    else:\n",
    "        numpy_type = False\n",
    "        if img.ndim == 2:\n",
    "            img = img.unsqueeze(0)\n",
    "            squeeze_flag = True\n",
    "\n",
    "    in_c, in_h, in_w = img.size()\n",
    "    out_h, out_w = math.ceil(in_h * scale), math.ceil(in_w * scale)\n",
    "    kernel_width = 4\n",
    "    kernel = 'cubic'\n",
    "\n",
    "    # get weights and indices\n",
    "    weights_h, indices_h, sym_len_hs, sym_len_he = calculate_weights_indices(in_h, out_h, scale, kernel, kernel_width,\n",
    "                                                                             antialiasing)\n",
    "    weights_w, indices_w, sym_len_ws, sym_len_we = calculate_weights_indices(in_w, out_w, scale, kernel, kernel_width,\n",
    "                                                                             antialiasing)\n",
    "    # process H dimension\n",
    "    # symmetric copying\n",
    "    img_aug = torch.FloatTensor(in_c, in_h + sym_len_hs + sym_len_he, in_w)\n",
    "    img_aug.narrow(1, sym_len_hs, in_h).copy_(img)\n",
    "\n",
    "    sym_patch = img[:, :sym_len_hs, :]\n",
    "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
    "    img_aug.narrow(1, 0, sym_len_hs).copy_(sym_patch_inv)\n",
    "\n",
    "    sym_patch = img[:, -sym_len_he:, :]\n",
    "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
    "    img_aug.narrow(1, sym_len_hs + in_h, sym_len_he).copy_(sym_patch_inv)\n",
    "\n",
    "    out_1 = torch.FloatTensor(in_c, out_h, in_w)\n",
    "    kernel_width = weights_h.size(1)\n",
    "    for i in range(out_h):\n",
    "        idx = int(indices_h[i][0])\n",
    "        for j in range(in_c):\n",
    "            out_1[j, i, :] = img_aug[j, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_h[i])\n",
    "\n",
    "    # process W dimension\n",
    "    # symmetric copying\n",
    "    out_1_aug = torch.FloatTensor(in_c, out_h, in_w + sym_len_ws + sym_len_we)\n",
    "    out_1_aug.narrow(2, sym_len_ws, in_w).copy_(out_1)\n",
    "\n",
    "    sym_patch = out_1[:, :, :sym_len_ws]\n",
    "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
    "    out_1_aug.narrow(2, 0, sym_len_ws).copy_(sym_patch_inv)\n",
    "\n",
    "    sym_patch = out_1[:, :, -sym_len_we:]\n",
    "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
    "    out_1_aug.narrow(2, sym_len_ws + in_w, sym_len_we).copy_(sym_patch_inv)\n",
    "\n",
    "    out_2 = torch.FloatTensor(in_c, out_h, out_w)\n",
    "    kernel_width = weights_w.size(1)\n",
    "    for i in range(out_w):\n",
    "        idx = int(indices_w[i][0])\n",
    "        for j in range(in_c):\n",
    "            out_2[j, :, i] = out_1_aug[j, :, idx:idx + kernel_width].mv(weights_w[i])\n",
    "\n",
    "    if squeeze_flag:\n",
    "        out_2 = out_2.squeeze(0)\n",
    "    if numpy_type:\n",
    "        out_2 = out_2.numpy()\n",
    "        if not squeeze_flag:\n",
    "            out_2 = out_2.transpose(1, 2, 0)\n",
    "\n",
    "    return out_2\n",
    "\n",
    "\n",
    "def _convert_input_type_range(img):\n",
    "    \"\"\"Convert the type and range of the input image.\n",
    "    It converts the input image to np.float32 type and range of [0, 1].\n",
    "    It is mainly used for pre-processing the input image in colorspace\n",
    "    conversion functions such as rgb2ycbcr and ycbcr2rgb.\n",
    "    Args:\n",
    "        img (ndarray): The input image. It accepts:\n",
    "            1. np.uint8 type with range [0, 255];\n",
    "            2. np.float32 type with range [0, 1].\n",
    "    Returns:\n",
    "        (ndarray): The converted image with type of np.float32 and range of\n",
    "            [0, 1].\n",
    "    \"\"\"\n",
    "    img_type = img.dtype\n",
    "    img = img.astype(np.float32)\n",
    "    if img_type == np.float32:\n",
    "        pass\n",
    "    elif img_type == np.uint8:\n",
    "        img /= 255.\n",
    "    else:\n",
    "        raise TypeError(f'The img type should be np.float32 or np.uint8, but got {img_type}')\n",
    "    return img\n",
    "\n",
    "\n",
    "def _convert_output_type_range(img, dst_type):\n",
    "    \"\"\"Convert the type and range of the image according to dst_type.\n",
    "    It converts the image to desired type and range. If `dst_type` is np.uint8,\n",
    "    images will be converted to np.uint8 type with range [0, 255]. If\n",
    "    `dst_type` is np.float32, it converts the image to np.float32 type with\n",
    "    range [0, 1].\n",
    "    It is mainly used for post-processing images in colorspace conversion\n",
    "    functions such as rgb2ycbcr and ycbcr2rgb.\n",
    "    Args:\n",
    "        img (ndarray): The image to be converted with np.float32 type and\n",
    "            range [0, 255].\n",
    "        dst_type (np.uint8 | np.float32): If dst_type is np.uint8, it\n",
    "            converts the image to np.uint8 type with range [0, 255]. If\n",
    "            dst_type is np.float32, it converts the image to np.float32 type\n",
    "            with range [0, 1].\n",
    "    Returns:\n",
    "        (ndarray): The converted image with desired type and range.\n",
    "    \"\"\"\n",
    "    if dst_type not in (np.uint8, np.float32):\n",
    "        raise TypeError(f'The dst_type should be np.float32 or np.uint8, but got {dst_type}')\n",
    "    if dst_type == np.uint8:\n",
    "        img = img.round()\n",
    "    else:\n",
    "        img /= 255.\n",
    "    return img.astype(dst_type)\n",
    "\n",
    "\n",
    "\n",
    "def rgb2ycbcr(img, y_only=False):\n",
    "    \"\"\"Convert a RGB image to YCbCr image.\n",
    "    This function produces the same results as Matlab's `rgb2ycbcr` function.\n",
    "    It implements the ITU-R BT.601 conversion for standard-definition\n",
    "    television. See more details in\n",
    "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
    "    It differs from a similar function in cv2.cvtColor: `RGB <-> YCrCb`.\n",
    "    In OpenCV, it implements a JPEG conversion. See more details in\n",
    "    https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion.\n",
    "    Args:\n",
    "        img (ndarray): The input image. It accepts:\n",
    "            1. np.uint8 type with range [0, 255];\n",
    "            2. np.float32 type with range [0, 1].\n",
    "        y_only (bool): Whether to only return Y channel. Default: False.\n",
    "    Returns:\n",
    "        ndarray: The converted YCbCr image. The output image has the same type\n",
    "            and range as input image.\n",
    "    \"\"\"\n",
    "    img_type = img.dtype\n",
    "    img = _convert_input_type_range(img)\n",
    "    if y_only:\n",
    "        out_img = np.dot(img, [65.481, 128.553, 24.966]) + 16.0\n",
    "    else:\n",
    "        out_img = np.matmul(\n",
    "            img, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786], [24.966, 112.0, -18.214]]) + [16, 128, 128]\n",
    "    out_img = _convert_output_type_range(out_img, img_type)\n",
    "    return out_img\n",
    "\n",
    "\n",
    "def bgr2ycbcr(img, y_only=False):\n",
    "    \"\"\"Convert a BGR image to YCbCr image.\n",
    "    The bgr version of rgb2ycbcr.\n",
    "    It implements the ITU-R BT.601 conversion for standard-definition\n",
    "    television. See more details in\n",
    "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
    "    It differs from a similar function in cv2.cvtColor: `BGR <-> YCrCb`.\n",
    "    In OpenCV, it implements a JPEG conversion. See more details in\n",
    "    https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion.\n",
    "    Args:\n",
    "        img (ndarray): The input image. It accepts:\n",
    "            1. np.uint8 type with range [0, 255];\n",
    "            2. np.float32 type with range [0, 1].\n",
    "        y_only (bool): Whether to only return Y channel. Default: False.\n",
    "    Returns:\n",
    "        ndarray: The converted YCbCr image. The output image has the same type\n",
    "            and range as input image.\n",
    "    \"\"\"\n",
    "    img_type = img.dtype\n",
    "    img = _convert_input_type_range(img)\n",
    "    if y_only:\n",
    "        out_img = np.dot(img, [24.966, 128.553, 65.481]) + 16.0\n",
    "    else:\n",
    "        out_img = np.matmul(\n",
    "            img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786], [65.481, -37.797, 112.0]]) + [16, 128, 128]\n",
    "    out_img = _convert_output_type_range(out_img, img_type)\n",
    "    return out_img\n",
    "\n",
    "def ycbcr2rgb(img):\n",
    "    \"\"\"Convert a YCbCr image to RGB image.\n",
    "    This function produces the same results as Matlab's ycbcr2rgb function.\n",
    "    It implements the ITU-R BT.601 conversion for standard-definition\n",
    "    television. See more details in\n",
    "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
    "    It differs from a similar function in cv2.cvtColor: `YCrCb <-> RGB`.\n",
    "    In OpenCV, it implements a JPEG conversion. See more details in\n",
    "    https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion.\n",
    "    Args:\n",
    "        img (ndarray): The input image. It accepts:\n",
    "            1. np.uint8 type with range [0, 255];\n",
    "            2. np.float32 type with range [0, 1].\n",
    "    Returns:\n",
    "        ndarray: The converted RGB image. The output image has the same type\n",
    "            and range as input image.\n",
    "    \"\"\"\n",
    "    img_type = img.dtype\n",
    "    img = _convert_input_type_range(img) * 255\n",
    "    out_img = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621], [0, -0.00153632, 0.00791071],\n",
    "                              [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]  # noqa: E126\n",
    "    out_img = _convert_output_type_range(out_img, img_type)\n",
    "    return out_img\n",
    "\n",
    "\n",
    "def to_y_channel(img):\n",
    "    \"\"\"Change to Y channel of YCbCr.\n",
    "    Args:\n",
    "        img (ndarray): Images with range [0, 255].\n",
    "    Returns:\n",
    "        (ndarray): Images with range [0, 255] (float type) without round.\n",
    "    \"\"\"\n",
    "    img = img.astype(np.float32) / 255.\n",
    "    if img.ndim == 3 and img.shape[2] == 3:\n",
    "        img = bgr2ycbcr(img, y_only=True)\n",
    "        img = img[..., None]\n",
    "    return img * 255.\n",
    "\n",
    "\n",
    "def reorder_image(img, input_order='HWC'):\n",
    "    \"\"\"Reorder images to 'HWC' order.\n",
    "    If the input_order is (h, w), return (h, w, 1);\n",
    "    If the input_order is (c, h, w), return (h, w, c);\n",
    "    If the input_order is (h, w, c), return as it is.\n",
    "    Args:\n",
    "        img (ndarray): Input image.\n",
    "        input_order (str): Whether the input order is 'HWC' or 'CHW'.\n",
    "            If the input image shape is (h, w), input_order will not have\n",
    "            effects. Default: 'HWC'.\n",
    "    Returns:\n",
    "        ndarray: reordered image.\n",
    "    \"\"\"\n",
    "\n",
    "    if input_order not in ['HWC', 'CHW']:\n",
    "        raise ValueError(f\"Wrong input_order {input_order}. Supported input_orders are 'HWC' and 'CHW'\")\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[..., None]\n",
    "    if input_order == 'CHW':\n",
    "        img = img.transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def rgb2ycbcr_pt(img, y_only=False):\n",
    "    \"\"\"Convert RGB images to YCbCr images (PyTorch version).\n",
    "    It implements the ITU-R BT.601 conversion for standard-definition television. See more details in\n",
    "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
    "    Args:\n",
    "        img (Tensor): Images with shape (n, 3, h, w), the range [0, 1], float, RGB format.\n",
    "         y_only (bool): Whether to only return Y channel. Default: False.\n",
    "    Returns:\n",
    "        (Tensor): converted images with the shape (n, 3/1, h, w), the range [0, 1], float.\n",
    "    \"\"\"\n",
    "    if y_only:\n",
    "        weight = torch.tensor([[65.481], [128.553], [24.966]]).to(img)\n",
    "        out_img = torch.matmul(img.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + 16.0\n",
    "    else:\n",
    "        weight = torch.tensor([[65.481, -37.797, 112.0], [128.553, -74.203, -93.786], [24.966, 112.0, -18.214]]).to(img)\n",
    "        bias = torch.tensor([16, 128, 128]).view(1, 3, 1, 1).to(img)\n",
    "        out_img = torch.matmul(img.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + bias\n",
    "\n",
    "    out_img = out_img / 255.\n",
    "    return\n",
    "\n",
    "def tensor2img(tensor):\n",
    "    im = (255. * tensor).data.cpu().numpy()\n",
    "    # clamp\n",
    "    im[im > 255] = 255\n",
    "    im[im < 0] = 0\n",
    "    im = im.astype(np.uint8)\n",
    "    return im\n",
    "\n",
    "def img2tensor(img):\n",
    "    img = (img / 255.).astype('float32')\n",
    "    if img.ndim ==2:\n",
    "        img = np.expand_dims(np.expand_dims(img, axis = 0),axis=0)\n",
    "    else:\n",
    "        img = np.transpose(img, (2, 0, 1))  # C, H, W\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "    tensor = torch.from_numpy(img)\n",
    "    return tensor\n",
    "\n",
    "def estimate_aggd_param(block):\n",
    "    \"\"\"Estimate AGGD (Asymmetric Generalized Gaussian Distribution) parameters.\n",
    "    Args:\n",
    "        block (ndarray): 2D Image block.\n",
    "    Returns:\n",
    "        tuple: alpha (float), beta_l (float) and beta_r (float) for the AGGD\n",
    "            distribution (Estimating the parames in Equation 7 in the paper).\n",
    "    \"\"\"\n",
    "    block = block.flatten()\n",
    "    gam = np.arange(0.2, 10.001, 0.001)  # len = 9801\n",
    "    gam_reciprocal = np.reciprocal(gam)\n",
    "    r_gam = np.square(gamma(gam_reciprocal * 2)) / (gamma(gam_reciprocal) * gamma(gam_reciprocal * 3))\n",
    "\n",
    "    left_std = np.sqrt(np.mean(block[block < 0]**2))\n",
    "    right_std = np.sqrt(np.mean(block[block > 0]**2))\n",
    "    gammahat = left_std / right_std\n",
    "    rhat = (np.mean(np.abs(block)))**2 / np.mean(block**2)\n",
    "    rhatnorm = (rhat * (gammahat**3 + 1) * (gammahat + 1)) / ((gammahat**2 + 1)**2)\n",
    "    array_position = np.argmin((r_gam - rhatnorm)**2)\n",
    "\n",
    "    alpha = gam[array_position]\n",
    "    beta_l = left_std * np.sqrt(gamma(1 / alpha) / gamma(3 / alpha))\n",
    "    beta_r = right_std * np.sqrt(gamma(1 / alpha) / gamma(3 / alpha))\n",
    "    return (alpha, beta_l, beta_r)\n",
    "\n",
    "\n",
    "def compute_feature(block):\n",
    "    \"\"\"Compute features.\n",
    "    Args:\n",
    "        block (ndarray): 2D Image block.\n",
    "    Returns:\n",
    "        list: Features with length of 18.\n",
    "    \"\"\"\n",
    "    feat = []\n",
    "    alpha, beta_l, beta_r = estimate_aggd_param(block)\n",
    "    feat.extend([alpha, (beta_l + beta_r) / 2])\n",
    "\n",
    "    # distortions disturb the fairly regular structure of natural images.\n",
    "    # This deviation can be captured by analyzing the sample distribution of\n",
    "    # the products of pairs of adjacent coefficients computed along\n",
    "    # horizontal, vertical and diagonal orientations.\n",
    "    shifts = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
    "    for i in range(len(shifts)):\n",
    "        shifted_block = np.roll(block, shifts[i], axis=(0, 1))\n",
    "        alpha, beta_l, beta_r = estimate_aggd_param(block * shifted_block)\n",
    "        # Eq. 8\n",
    "        mean = (beta_r - beta_l) * (gamma(2 / alpha) / gamma(1 / alpha))\n",
    "        feat.extend([alpha, mean, beta_l, beta_r])\n",
    "    return feat\n",
    "\n",
    "\n",
    "def niqe(img, mu_pris_param, cov_pris_param, gaussian_window, block_size_h=96, block_size_w=96):\n",
    "    \"\"\"Calculate NIQE (Natural Image Quality Evaluator) metric.\n",
    "    ``Paper: Making a \"Completely Blind\" Image Quality Analyzer``\n",
    "    This implementation could produce almost the same results as the official\n",
    "    MATLAB codes: http://live.ece.utexas.edu/research/quality/niqe_release.zip\n",
    "    Note that we do not include block overlap height and width, since they are\n",
    "    always 0 in the official implementation.\n",
    "    For good performance, it is advisable by the official implementation to\n",
    "    divide the distorted image in to the same size patched as used for the\n",
    "    construction of multivariate Gaussian model.\n",
    "    Args:\n",
    "        img (ndarray): Input image whose quality needs to be computed. The\n",
    "            image must be a gray or Y (of YCbCr) image with shape (h, w).\n",
    "            Range [0, 255] with float type.\n",
    "        mu_pris_param (ndarray): Mean of a pre-defined multivariate Gaussian\n",
    "            model calculated on the pristine dataset.\n",
    "        cov_pris_param (ndarray): Covariance of a pre-defined multivariate\n",
    "            Gaussian model calculated on the pristine dataset.\n",
    "        gaussian_window (ndarray): A 7x7 Gaussian window used for smoothing the\n",
    "            image.\n",
    "        block_size_h (int): Height of the blocks in to which image is divided.\n",
    "            Default: 96 (the official recommended value).\n",
    "        block_size_w (int): Width of the blocks in to which image is divided.\n",
    "            Default: 96 (the official recommended value).\n",
    "    \"\"\"\n",
    "    assert img.ndim == 2, ('Input image must be a gray or Y (of YCbCr) image with shape (h, w).')\n",
    "    # crop image\n",
    "    h, w = img.shape\n",
    "    num_block_h = math.floor(h / block_size_h)\n",
    "    num_block_w = math.floor(w / block_size_w)\n",
    "    img = img[0:num_block_h * block_size_h, 0:num_block_w * block_size_w]\n",
    "\n",
    "    distparam = []  # dist param is actually the multiscale features\n",
    "    for scale in (1, 2):  # perform on two scales (1, 2)\n",
    "        mu = convolve(img, gaussian_window, mode='nearest')\n",
    "        sigma = np.sqrt(np.abs(convolve(np.square(img), gaussian_window, mode='nearest') - np.square(mu)))\n",
    "        # normalize, as in Eq. 1 in the paper\n",
    "        img_nomalized = (img - mu) / (sigma + 1)\n",
    "\n",
    "        feat = []\n",
    "        for idx_w in range(num_block_w):\n",
    "            for idx_h in range(num_block_h):\n",
    "                # process ecah block\n",
    "                block = img_nomalized[idx_h * block_size_h // scale:(idx_h + 1) * block_size_h // scale,\n",
    "                                      idx_w * block_size_w // scale:(idx_w + 1) * block_size_w // scale]\n",
    "                feat.append(compute_feature(block))\n",
    "\n",
    "        distparam.append(np.array(feat))\n",
    "\n",
    "        if scale == 1:\n",
    "            img = imresize(img / 255., scale=0.5, antialiasing=True)\n",
    "            img = img * 255.\n",
    "\n",
    "    distparam = np.concatenate(distparam, axis=1)\n",
    "\n",
    "    # fit a MVG (multivariate Gaussian) model to distorted patch features\n",
    "    mu_distparam = np.nanmean(distparam, axis=0)\n",
    "    # use nancov. ref: https://ww2.mathworks.cn/help/stats/nancov.html\n",
    "    distparam_no_nan = distparam[~np.isnan(distparam).any(axis=1)]\n",
    "    cov_distparam = np.cov(distparam_no_nan, rowvar=False)\n",
    "\n",
    "    # compute niqe quality, Eq. 10 in the paper\n",
    "    invcov_param = np.linalg.pinv((cov_pris_param + cov_distparam) / 2)\n",
    "    quality = np.matmul(\n",
    "        np.matmul((mu_pris_param - mu_distparam), invcov_param), np.transpose((mu_pris_param - mu_distparam)))\n",
    "\n",
    "    quality = np.sqrt(quality)\n",
    "    quality = float(np.squeeze(quality))\n",
    "    return quality\n",
    "\n",
    "\n",
    "def calculate_niqe(img, crop_border=0,input_order='HWC', convert_to='y', **kwargs):\n",
    "    \"\"\"Calculate NIQE (Natural Image Quality Evaluator) metric.\n",
    "    ``Paper: Making a \"Completely Blind\" Image Quality Analyzer``\n",
    "    This implementation could produce almost the same results as the official\n",
    "    MATLAB codes: http://live.ece.utexas.edu/research/quality/niqe_release.zip\n",
    "    > MATLAB R2021a result for tests/data/baboon.png: 5.72957338 (5.7296)\n",
    "    > Our re-implementation result for tests/data/baboon.png: 5.7295763 (5.7296)\n",
    "    We use the official params estimated from the pristine dataset.\n",
    "    We use the recommended block size (96, 96) without overlaps.\n",
    "    Args:\n",
    "        img (ndarray): Input image whose quality needs to be computed.\n",
    "            The input image must be in range [0, 255] with float/int type.\n",
    "            The input_order of image can be 'HW' or 'HWC' or 'CHW'. (BGR order)\n",
    "            If the input order is 'HWC' or 'CHW', it will be converted to gray\n",
    "            or Y (of YCbCr) image according to the ``convert_to`` argument.\n",
    "        crop_border (int): Cropped pixels in each edge of an image. These\n",
    "            pixels are not involved in the metric calculation.\n",
    "        input_order (str): Whether the input order is 'HW', 'HWC' or 'CHW'.\n",
    "            Default: 'HWC'.\n",
    "        convert_to (str): Whether converted to 'y' (of MATLAB YCbCr) or 'gray'.\n",
    "            Default: 'y'.\n",
    "    Returns:\n",
    "        float: NIQE result.\n",
    "    \"\"\"\n",
    "    # ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    # we use the official params estimated from the pristine dataset.\n",
    "    niqe_pris_params = np.load(r\"D:\\contrast enhancement\\just\\niqe_pris_params.npz\")\n",
    "    mu_pris_param = niqe_pris_params['mu_pris_param']\n",
    "    cov_pris_param = niqe_pris_params['cov_pris_param']\n",
    "    gaussian_window = niqe_pris_params['gaussian_window']\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    if input_order != 'HW':\n",
    "        img = reorder_image(img, input_order=input_order)\n",
    "        if convert_to == 'y':\n",
    "            img = to_y_channel(img)\n",
    "        elif convert_to == 'gray':\n",
    "            img = cv2.cvtColor(img / 255., cv2.COLOR_BGR2GRAY) * 255.\n",
    "        img = np.squeeze(img)\n",
    "\n",
    "    if crop_border != 0:\n",
    "        img = img[crop_border:-crop_border, crop_border:-crop_border]\n",
    "\n",
    "    # round is necessary for being consistent with MATLAB's result\n",
    "    img = img.round()\n",
    "\n",
    "    niqe_result = niqe(img, mu_pris_param, cov_pris_param, gaussian_window)\n",
    "\n",
    "    return niqe_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9b929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imquality.brisque as brisque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76db9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(im_dir):\n",
    "    avg_niqe = 0\n",
    "    n = 0\n",
    "    avg_brisque = 0\n",
    "        \n",
    "    for item in tqdm(sorted(glob.glob(im_dir))):\n",
    "        n += 1\n",
    "        \n",
    "        im1 = Image.open(item).convert('RGB')\n",
    "        im1 = np.array(im1) \n",
    "        score_brisque = brisque.score(im1) \n",
    "        im1 = np.array(im1)\n",
    "        score_niqe = calculate_niqe(im1)\n",
    "        \n",
    "        \n",
    "        avg_brisque += score_brisque\n",
    "        avg_niqe += score_niqe\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    avg_brisque = avg_brisque / n\n",
    "    avg_niqe = avg_niqe / n\n",
    "    return avg_niqe, avg_brisque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d418d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 14%|█▍        | 1/7 [00:01<00:06,  1.00s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 29%|██▊       | 2/7 [00:04<00:12,  2.43s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 43%|████▎     | 3/7 [00:05<00:07,  1.88s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 57%|█████▋    | 4/7 [00:07<00:06,  2.02s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 71%|███████▏  | 5/7 [00:08<00:03,  1.54s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 86%|████████▌ | 6/7 [00:08<00:01,  1.13s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      "100%|██████████| 7/7 [00:09<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7791275848414956\n",
      "17.85127923490239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "im_dir =  r'D:\\new_unpaired\\NPE_ENHANCED/*.jpg'\n",
    "avg_niqe, avg_brisque = metrics(im_dir)\n",
    "print(avg_niqe)\n",
    "print(avg_brisque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12adbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 14%|█▍        | 1/7 [00:01<00:06,  1.09s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 29%|██▊       | 2/7 [00:04<00:12,  2.45s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 43%|████▎     | 3/7 [00:05<00:07,  1.90s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 57%|█████▋    | 4/7 [00:07<00:06,  2.01s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 71%|███████▏  | 5/7 [00:08<00:03,  1.54s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 86%|████████▌ | 6/7 [00:08<00:01,  1.14s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      "100%|██████████| 7/7 [00:09<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.743864348573507\n",
      "18.146735010371057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "im_dir =  r'D:\\new_unpaired\\NPE_NOADAPTIVEMUTATION/*.jpg'\n",
    "avg_niqe, avg_brisque = metrics(im_dir)\n",
    "print(avg_niqe)\n",
    "print(avg_brisque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c4f281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 14%|█▍        | 1/7 [00:01<00:06,  1.09s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 29%|██▊       | 2/7 [00:04<00:12,  2.42s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 43%|████▎     | 3/7 [00:05<00:07,  1.86s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 57%|█████▋    | 4/7 [00:07<00:05,  1.99s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 71%|███████▏  | 5/7 [00:08<00:03,  1.52s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 86%|████████▌ | 6/7 [00:08<00:01,  1.12s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      "100%|██████████| 7/7 [00:09<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7209310242021822\n",
      "17.927558974416737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "im_dir =  r'D:\\new_unpaired\\NPE_NOLOCALSEARCH/*.jpg'\n",
    "avg_niqe, avg_brisque = metrics(im_dir)\n",
    "print(avg_niqe)\n",
    "print(avg_brisque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5c696ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 14%|█▍        | 1/7 [00:00<00:05,  1.02it/s]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 29%|██▊       | 2/7 [00:04<00:11,  2.21s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 43%|████▎     | 3/7 [00:05<00:06,  1.71s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 57%|█████▋    | 4/7 [00:07<00:05,  1.83s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 71%|███████▏  | 5/7 [00:07<00:02,  1.41s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      " 86%|████████▌ | 6/7 [00:08<00:01,  1.05s/it]C:\\Users\\FA007\\.conda\\envs\\new4\\lib\\site-packages\\imquality\\brisque.py:45: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  self.image = skimage.color.rgb2gray(self.image)\n",
      "100%|██████████| 7/7 [00:09<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7840140038581147\n",
      "17.67079036503578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "im_dir =  r'D:\\new_unpaired\\NPE_ENHANCED_NSGA2ONLY/*.jpg'\n",
    "avg_niqe, avg_brisque = metrics(im_dir)\n",
    "print(avg_niqe)\n",
    "print(avg_brisque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ad46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
