{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8412bb33-5857-4c0d-ba48-eb3b5764b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FA007\\.conda\\envs\\new2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import safetensors.torch as sf\n",
    "from huggingface_hub import hf_hub_download\n",
    "import argparse\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import platform\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1b1427-4d29-4562-9773-74be0eb910f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pi = 3.141592653589793\n",
    "\n",
    "class RGB_HVI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RGB_HVI, self).__init__()\n",
    "        self.density_k = torch.nn.Parameter(torch.full([1],0.2)) # k is reciprocal to the paper mentioned\n",
    "        self.gated = False\n",
    "        self.gated2= False\n",
    "        self.alpha = 1.0\n",
    "        self.alpha_s = 1.3\n",
    "        self.this_k = 0\n",
    "        \n",
    "    def HVIT(self, img):\n",
    "        eps = 1e-8\n",
    "        device = img.device\n",
    "        dtypes = img.dtype\n",
    "        hue = torch.Tensor(img.shape[0], img.shape[2], img.shape[3]).to(device).to(dtypes)\n",
    "        value = img.max(1)[0].to(dtypes)\n",
    "        img_min = img.min(1)[0].to(dtypes)\n",
    "        hue[img[:,2]==value] = 4.0 + ( (img[:,0]-img[:,1]) / (value - img_min + eps)) [img[:,2]==value]\n",
    "        hue[img[:,1]==value] = 2.0 + ( (img[:,2]-img[:,0]) / (value - img_min + eps)) [img[:,1]==value]\n",
    "        hue[img[:,0]==value] = (0.0 + ((img[:,1]-img[:,2]) / (value - img_min + eps)) [img[:,0]==value]) % 6\n",
    "\n",
    "        hue[img.min(1)[0]==value] = 0.0\n",
    "        hue = hue/6.0\n",
    "\n",
    "        saturation = (value - img_min ) / (value + eps )\n",
    "        saturation[value==0] = 0\n",
    "\n",
    "        hue = hue.unsqueeze(1)\n",
    "        saturation = saturation.unsqueeze(1)\n",
    "        value = value.unsqueeze(1)\n",
    "        \n",
    "        k = self.density_k\n",
    "        self.this_k = k.item()\n",
    "        \n",
    "        color_sensitive = ((value * 0.5 * pi).sin() + eps).pow(k)\n",
    "        ch = (2.0 * pi * hue).cos()\n",
    "        cv = (2.0 * pi * hue).sin()\n",
    "        H = color_sensitive * saturation * ch\n",
    "        V = color_sensitive * saturation * cv\n",
    "        I = value\n",
    "        xyz = torch.cat([H, V, I],dim=1)\n",
    "        return xyz\n",
    "    \n",
    "    def PHVIT(self, img):\n",
    "        eps = 1e-8\n",
    "        H,V,I = img[:,0,:,:],img[:,1,:,:],img[:,2,:,:]\n",
    "        \n",
    "        # clip\n",
    "        H = torch.clamp(H,-1,1)\n",
    "        V = torch.clamp(V,-1,1)\n",
    "        I = torch.clamp(I,0,1)\n",
    "        \n",
    "        v = I\n",
    "        k = self.this_k\n",
    "        color_sensitive = ((v * 0.5 * pi).sin() + eps).pow(k)\n",
    "        H = (H) / (color_sensitive + eps)\n",
    "        V = (V) / (color_sensitive + eps)\n",
    "        H = torch.clamp(H,-1,1)\n",
    "        V = torch.clamp(V,-1,1)\n",
    "        h = torch.atan2(V + eps,H + eps) / (2*pi)\n",
    "        h = h%1\n",
    "        s = torch.sqrt(H**2 + V**2 + eps)\n",
    "        \n",
    "        if self.gated:\n",
    "            s = s * self.alpha_s\n",
    "        \n",
    "        s = torch.clamp(s,0,1)\n",
    "        v = torch.clamp(v,0,1)\n",
    "        \n",
    "        r = torch.zeros_like(h)\n",
    "        g = torch.zeros_like(h)\n",
    "        b = torch.zeros_like(h)\n",
    "        \n",
    "        hi = torch.floor(h * 6.0)\n",
    "        f = h * 6.0 - hi\n",
    "        p = v * (1. - s)\n",
    "        q = v * (1. - (f * s))\n",
    "        t = v * (1. - ((1. - f) * s))\n",
    "        \n",
    "        hi0 = hi==0\n",
    "        hi1 = hi==1\n",
    "        hi2 = hi==2\n",
    "        hi3 = hi==3\n",
    "        hi4 = hi==4\n",
    "        hi5 = hi==5\n",
    "        \n",
    "        r[hi0] = v[hi0]\n",
    "        g[hi0] = t[hi0]\n",
    "        b[hi0] = p[hi0]\n",
    "        \n",
    "        r[hi1] = q[hi1]\n",
    "        g[hi1] = v[hi1]\n",
    "        b[hi1] = p[hi1]\n",
    "        \n",
    "        r[hi2] = p[hi2]\n",
    "        g[hi2] = v[hi2]\n",
    "        b[hi2] = t[hi2]\n",
    "        \n",
    "        r[hi3] = p[hi3]\n",
    "        g[hi3] = q[hi3]\n",
    "        b[hi3] = v[hi3]\n",
    "        \n",
    "        r[hi4] = t[hi4]\n",
    "        g[hi4] = p[hi4]\n",
    "        b[hi4] = v[hi4]\n",
    "        \n",
    "        r[hi5] = v[hi5]\n",
    "        g[hi5] = p[hi5]\n",
    "        b[hi5] = q[hi5]\n",
    "                \n",
    "        r = r.unsqueeze(1)\n",
    "        g = g.unsqueeze(1)\n",
    "        b = b.unsqueeze(1)\n",
    "        rgb = torch.cat([r, g, b], dim=1)\n",
    "        if self.gated2:\n",
    "            rgb = rgb * self.alpha\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b914d069-12dc-4bb9-8399-14fc9c525a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "# Cross Attention Block\n",
    "class CAB(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        super(CAB, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        self.q_dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)\n",
    "        self.kv = nn.Conv2d(dim, dim*2, kernel_size=1, bias=bias)\n",
    "        self.kv_dwconv = nn.Conv2d(dim*2, dim*2, kernel_size=3, stride=1, padding=1, groups=dim*2, bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        q = self.q_dwconv(self.q(x))\n",
    "        kv = self.kv_dwconv(self.kv(y))\n",
    "        k, v = kv.chunk(2, dim=1)\n",
    "\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = nn.functional.softmax(attn,dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Intensity Enhancement Layer\n",
    "class IEL(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor=2.66, bias=False):\n",
    "        super(IEL, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "        \n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "        self.dwconv1 = nn.Conv2d(hidden_features, hidden_features, kernel_size=3, stride=1, padding=1, groups=hidden_features, bias=bias)\n",
    "        self.dwconv2 = nn.Conv2d(hidden_features, hidden_features, kernel_size=3, stride=1, padding=1, groups=hidden_features, bias=bias)\n",
    "       \n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.Tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x1 = self.Tanh(self.dwconv1(x1)) + x1\n",
    "        x2 = self.Tanh(self.dwconv2(x2)) + x2\n",
    "        x = x1 * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "  \n",
    "  \n",
    "# Lightweight Cross Attention\n",
    "class HV_LCA(nn.Module):\n",
    "    def __init__(self, dim,num_heads, bias=False):\n",
    "        super(HV_LCA, self).__init__()\n",
    "        self.gdfn = IEL(dim) # IEL and CDL have same structure\n",
    "        self.norm = LayerNorm(dim)\n",
    "        self.ffn = CAB(dim, num_heads, bias)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = x + self.ffn(self.norm(x),self.norm(y))\n",
    "        x = self.gdfn(self.norm(x))\n",
    "        return x\n",
    "    \n",
    "class I_LCA(nn.Module):\n",
    "    def __init__(self, dim,num_heads, bias=False):\n",
    "        super(I_LCA, self).__init__()\n",
    "        self.norm = LayerNorm(dim)\n",
    "        self.gdfn = IEL(dim)\n",
    "        self.ffn = CAB(dim, num_heads, bias=bias)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = x + self.ffn(self.norm(x),self.norm(y))\n",
    "        x = x + self.gdfn(self.norm(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d10a9ea-38b0-4811-8ff3-3aa67a988dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_first\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "class NormDownsample(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,scale=0.5,use_norm=False):\n",
    "        super(NormDownsample, self).__init__()\n",
    "        self.use_norm=use_norm\n",
    "        if self.use_norm:\n",
    "            self.norm=LayerNorm(out_ch)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch,kernel_size=3,stride=1, padding=1, bias=False),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=scale))\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        x = self.prelu(x)\n",
    "        if self.use_norm:\n",
    "            x = self.norm(x)\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class NormUpsample(nn.Module):\n",
    "    def __init__(self, in_ch,out_ch,scale=2,use_norm=False):\n",
    "        super(NormUpsample, self).__init__()\n",
    "        self.use_norm=use_norm\n",
    "        if self.use_norm:\n",
    "            self.norm=LayerNorm(out_ch)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.up_scale = nn.Sequential(\n",
    "            nn.Conv2d(in_ch,out_ch,kernel_size=3,stride=1, padding=1, bias=False),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=scale))\n",
    "        self.up = nn.Conv2d(out_ch*2,out_ch,kernel_size=1,stride=1, padding=0, bias=False)\n",
    "            \n",
    "    def forward(self, x,y):\n",
    "        x = self.up_scale(x)\n",
    "        x = torch.cat([x, y],dim=1)\n",
    "        x = self.up(x)\n",
    "        x = self.prelu(x)\n",
    "        if self.use_norm:\n",
    "            return self.norm(x)\n",
    "        else:\n",
    "            return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f95b587-95f7-46a7-83a3-b67cc1a1b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "class CIDNet(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, \n",
    "                 channels=[36, 36, 72, 144],\n",
    "                 heads=[1, 2, 4, 8],\n",
    "                 norm=False\n",
    "        ):\n",
    "        super(CIDNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        [ch1, ch2, ch3, ch4] = channels\n",
    "        [head1, head2, head3, head4] = heads\n",
    "        \n",
    "        # HV_ways\n",
    "        self.HVE_block0 = nn.Sequential(\n",
    "            nn.ReplicationPad2d(1),\n",
    "            nn.Conv2d(3, ch1, 3, stride=1, padding=0,bias=False)\n",
    "            )\n",
    "        self.HVE_block1 = NormDownsample(ch1, ch2, use_norm = norm)\n",
    "        self.HVE_block2 = NormDownsample(ch2, ch3, use_norm = norm)\n",
    "        self.HVE_block3 = NormDownsample(ch3, ch4, use_norm = norm)\n",
    "        \n",
    "        self.HVD_block3 = NormUpsample(ch4, ch3, use_norm = norm)\n",
    "        self.HVD_block2 = NormUpsample(ch3, ch2, use_norm = norm)\n",
    "        self.HVD_block1 = NormUpsample(ch2, ch1, use_norm = norm)\n",
    "        self.HVD_block0 = nn.Sequential(\n",
    "            nn.ReplicationPad2d(1),\n",
    "            nn.Conv2d(ch1, 2, 3, stride=1, padding=0,bias=False)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # I_ways\n",
    "        self.IE_block0 = nn.Sequential(\n",
    "            nn.ReplicationPad2d(1),\n",
    "            nn.Conv2d(1, ch1, 3, stride=1, padding=0,bias=False),\n",
    "            )\n",
    "        self.IE_block1 = NormDownsample(ch1, ch2, use_norm = norm)\n",
    "        self.IE_block2 = NormDownsample(ch2, ch3, use_norm = norm)\n",
    "        self.IE_block3 = NormDownsample(ch3, ch4, use_norm = norm)\n",
    "        \n",
    "        self.ID_block3 = NormUpsample(ch4, ch3, use_norm=norm)\n",
    "        self.ID_block2 = NormUpsample(ch3, ch2, use_norm=norm)\n",
    "        self.ID_block1 = NormUpsample(ch2, ch1, use_norm=norm)\n",
    "        self.ID_block0 =  nn.Sequential(\n",
    "            nn.ReplicationPad2d(1),\n",
    "            nn.Conv2d(ch1, 1, 3, stride=1, padding=0,bias=False),\n",
    "            )\n",
    "        \n",
    "        self.HV_LCA1 = HV_LCA(ch2, head2)\n",
    "        self.HV_LCA2 = HV_LCA(ch3, head3)\n",
    "        self.HV_LCA3 = HV_LCA(ch4, head4)\n",
    "        self.HV_LCA4 = HV_LCA(ch4, head4)\n",
    "        self.HV_LCA5 = HV_LCA(ch3, head3)\n",
    "        self.HV_LCA6 = HV_LCA(ch2, head2)\n",
    "        \n",
    "        self.I_LCA1 = I_LCA(ch2, head2)\n",
    "        self.I_LCA2 = I_LCA(ch3, head3)\n",
    "        self.I_LCA3 = I_LCA(ch4, head4)\n",
    "        self.I_LCA4 = I_LCA(ch4, head4)\n",
    "        self.I_LCA5 = I_LCA(ch3, head3)\n",
    "        self.I_LCA6 = I_LCA(ch2, head2)\n",
    "        \n",
    "        self.trans = RGB_HVI()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dtypes = x.dtype\n",
    "        hvi = self.trans.HVIT(x)\n",
    "        i = hvi[:,2,:,:].unsqueeze(1).to(dtypes)\n",
    "        # low\n",
    "        i_enc0 = self.IE_block0(i)\n",
    "        i_enc1 = self.IE_block1(i_enc0)\n",
    "        hv_0 = self.HVE_block0(hvi)\n",
    "        hv_1 = self.HVE_block1(hv_0)\n",
    "        i_jump0 = i_enc0\n",
    "        hv_jump0 = hv_0\n",
    "        \n",
    "        i_enc2 = self.I_LCA1(i_enc1, hv_1)\n",
    "        hv_2 = self.HV_LCA1(hv_1, i_enc1)\n",
    "        v_jump1 = i_enc2\n",
    "        hv_jump1 = hv_2\n",
    "        i_enc2 = self.IE_block2(i_enc2)\n",
    "        hv_2 = self.HVE_block2(hv_2)\n",
    "        \n",
    "        i_enc3 = self.I_LCA2(i_enc2, hv_2)\n",
    "        hv_3 = self.HV_LCA2(hv_2, i_enc2)\n",
    "        v_jump2 = i_enc3\n",
    "        hv_jump2 = hv_3\n",
    "        i_enc3 = self.IE_block3(i_enc2)\n",
    "        hv_3 = self.HVE_block3(hv_2)\n",
    "        \n",
    "        i_enc4 = self.I_LCA3(i_enc3, hv_3)\n",
    "        hv_4 = self.HV_LCA3(hv_3, i_enc3)\n",
    "        \n",
    "        i_dec4 = self.I_LCA4(i_enc4,hv_4)\n",
    "        hv_4 = self.HV_LCA4(hv_4, i_enc4)\n",
    "        \n",
    "        hv_3 = self.HVD_block3(hv_4, hv_jump2)\n",
    "        i_dec3 = self.ID_block3(i_dec4, v_jump2)\n",
    "        i_dec2 = self.I_LCA5(i_dec3, hv_3)\n",
    "        hv_2 = self.HV_LCA5(hv_3, i_dec3)\n",
    "        \n",
    "        hv_2 = self.HVD_block2(hv_2, hv_jump1)\n",
    "        i_dec2 = self.ID_block2(i_dec3, v_jump1)\n",
    "        \n",
    "        i_dec1 = self.I_LCA6(i_dec2, hv_2)\n",
    "        hv_1 = self.HV_LCA6(hv_2, i_dec2)\n",
    "        \n",
    "        i_dec1 = self.ID_block1(i_dec1, i_jump0)\n",
    "        i_dec0 = self.ID_block0(i_dec1)\n",
    "        hv_1 = self.HVD_block1(hv_1, hv_jump0)\n",
    "        hv_0 = self.HVD_block0(hv_1)\n",
    "        \n",
    "        output_hvi = torch.cat([hv_0, i_dec0], dim=1) + hvi\n",
    "        output_rgb = self.trans.PHVIT(output_hvi)\n",
    "\n",
    "        return output_rgb\n",
    "    \n",
    "    def HVIT(self,x):\n",
    "        hvi = self.trans.HVIT(x)\n",
    "        return hvi\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb7c293-9396-4cbe-be79-6c1641c57d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0034-LSYD4O2202.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0035-dgw_048.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0291-IMG_0115.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0436-IMG_2583.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0452-IMG_1646.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0463-jmac_DSC2316.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0631-NKIM_MG_6442.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0648-IMG_5085.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0712-_DSC8911.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0752-20061213_134314__MG_3708.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0767-jn_20070824_0165.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0770-050703_161554__I2E9266.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0775-kme_423.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0811-20051224_165428__MG_0953.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0843-IMG_0009.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0877-_DGW6231.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0944-20061213_132310__MG_3646.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a0982-jmac_MG_1105.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1085-_DSC6188.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1099-IMG_3511.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1125-07-11-25-at-10h33m49s-_MG_6884.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1160-_MG_1159.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1194-IMG_0479.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1219-IMG_3770.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1268-jmac_MG_5989.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1319-IMG_2601.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1352-07-11-04-at-17h58m48s-_MG_4012.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1368-kme_086.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1439-IMG_0055.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1454-DSC_0028-1.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1606-kme_447.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1624-_DSC0158.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1659-IMG_4181.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1693-IMG_5069.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1701-jmac_DSC3938.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1717-kme_510.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1741-kme_305.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1775-dvf_006.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1789-jmac_DSC6275.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a1806-kme_342.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2031-WP_CRW_5715.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2106-20041009_164255__E6B5580.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2215-DSC_0018.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2224-MB_070908_032.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2236-jn_2007_12_10__BU_CRUISE_004.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2314-20080426_111248__MG_9227.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2398-051007_160936__I2E8858.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2416-_DGW6256.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2473-kme_303.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2499-_DSC1385.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2504-kme_370.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2560-MB_070908_079.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2713-kme_337.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2756-kme_506.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2761-kme_607.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2773-jmac_MG_4982.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2835-IMG_4968.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2879-IMG_0481.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a2881-20070514_162430__MG_7345.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3134-IMG_4250.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3193-KE_-2155.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3304-IMG_4322.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3311-IMG_4500.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3330-DSC_0048.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3360-IMG_0017.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3384-kme_524.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3411-_DGW6385.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3590-IMG_0622.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3697-07-11-24-at-16h05m35s-_MG_6729.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3732-_DGW6272.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3739-KE_-0068.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3741-KE_-8337.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3753-dgw_073.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3768-IMG_1641.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3790-IMG_5048.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3831-jmac_MG_5861.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a3959-dvf_057.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4062-IMG_1820.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4064-07-12-02-at-16h23m18s-_MG_9020.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4074-kme_428.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4090-IMG_4996.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4128-kme_388.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4225-Duggan_081109_3031.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4226-kme_0501.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4336-09-05-17-at-15h52m34s-_MG_9211.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4428-DSC_0031.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4454-DSC_0101.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4465-DSC_0051.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4501-DSC_0354.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4528-Duggan_090209_4971.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4557-DSC_0477.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4573-kme_0734.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4635-DSC_0017.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4662-Duggan_080115_4605.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4673-CRW_0009.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4706-DSC_0066.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4826-Duggan_080821_1199.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4828-Duggan_050710_1299.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4891-DSC_0069.jpg\n",
      "Saved: D:\\mit-5k-subset\\e_enhanced_cidnet\\a4943-20090327_at_11h17m20__MG_0987.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import hf_hub_download\n",
    "import safetensors.torch as sf\n",
    "\n",
    "# Simulate CLI args in notebook\n",
    "sys.argv = ['']\n",
    "\n",
    "# Argument parsing\n",
    "eval_parser = argparse.ArgumentParser(description='EvalHF')\n",
    "eval_parser.add_argument('--path', type=str, default=\"Fediory/HVI-CIDNet-LOLv1-wperc\",\n",
    "                         help='HuggingFace model repo (e.g. Fediory/HVI-CIDNet-LOLv1-wperc)')\n",
    "eval_parser.add_argument('--input_dir', type=str, default=r\"D:\\mit-5k-subset\\e\", help='Directory of input images')\n",
    "eval_parser.add_argument('--alpha_s', type=float, default=1.0)\n",
    "eval_parser.add_argument('--alpha_i', type=float, default=1.0)\n",
    "eval_parser.add_argument('--gamma', type=float, default=1.0)\n",
    "el = eval_parser.parse_args()\n",
    "\n",
    "# Model loader\n",
    "def from_pretrained(cls, pretrained_model_name_or_path: str):\n",
    "    model_id = str(pretrained_model_name_or_path)\n",
    "\n",
    "    config_file = hf_hub_download(repo_id=model_id, filename=\"config.json\", repo_type=\"model\")\n",
    "    config = None\n",
    "    if config_file is not None:\n",
    "        with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "    model_file = hf_hub_download(repo_id=model_id, filename=\"model.safetensors\", repo_type=\"model\")\n",
    "    state_dict = sf.load_file(model_file)\n",
    "    cls.load_state_dict(state_dict, strict=False) \n",
    "    return cls\n",
    "\n",
    "# Load model\n",
    "model = CIDNet().cuda()\n",
    "model = from_pretrained(cls=model, pretrained_model_name_or_path=el.path)\n",
    "model.eval()\n",
    "\n",
    "# Image enhancement logic\n",
    "pil2tensor = transforms.Compose([transforms.ToTensor()])\n",
    "output_folder = r\"D:\\mit-5k-subset\\e_enhanced_cidnet\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over all images\n",
    "for fname in os.listdir(el.input_dir):\n",
    "    if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "        img_path = os.path.join(el.input_dir, fname)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        input = pil2tensor(img)\n",
    "        \n",
    "        factor = 8\n",
    "        h, w = input.shape[1], input.shape[2]\n",
    "        H, W = ((h + factor) // factor) * factor, ((w + factor) // factor) * factor\n",
    "        padh = H - h if h % factor != 0 else 0\n",
    "        padw = W - w if w % factor != 0 else 0\n",
    "        input = F.pad(input.unsqueeze(0), (0, padw, 0, padh), 'reflect')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.trans.alpha_s = el.alpha_s\n",
    "            model.trans.alpha = el.alpha_i\n",
    "            model.trans.gated = True\n",
    "            model.trans.gated2 = True\n",
    "            output = model(input.cuda() ** el.gamma)\n",
    "\n",
    "        output = torch.clamp(output.cuda(), 0, 1)\n",
    "        output = output[:, :, :h, :w]\n",
    "        enhanced_img = transforms.ToPILImage()(output.squeeze(0))\n",
    "\n",
    "        save_path = os.path.join(output_folder, fname)\n",
    "        enhanced_img.save(save_path)\n",
    "        print(f\"Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5cc736-df40-47da-930c-b40968aa932b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
